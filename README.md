Ce projet porte sur la classification supervis√©e multi-classes d‚Äôindividus de manchots √† partir de caract√©ristiques biologiques et environnementales.
L‚Äôobjectif principal est de pr√©dire l‚Äôesp√®ce d‚Äôun manchot parmi trois esp√®ces :

Adelie Penguin (Pygoscelis adeliae)

Chinstrap Penguin (Pygoscelis antarctica)

Gentoo Penguin (Pygoscelis papua)

√† partir de variables morphologiques (bec, nageoires, masse corporelle), isotopiques et cat√©gorielles (√Æle, sexe).

Le jeu de donn√©es utilis√© est celui des Palmer Penguins, un dataset r√©el largement utilis√© pour l‚Äôapprentissage statistique et le machine learning, car il pr√©sente √† la fois :

des donn√©es continues,

des variables cat√©gorielles,

des valeurs manquantes,

et une structure biologique interpr√©table.

Ce projet vise √† mettre en ≈ìuvre une d√©marche compl√®te d‚Äôanalyse de donn√©es, depuis les donn√©es brutes jusqu‚Äô√† la comparaison entre m√©thodes param√©triques et non param√©triques.

Contenu du projet

Le projet est structur√© de mani√®re progressive et suit les √©tapes classiques d‚Äôun pipeline de data science.

1. Analyse des donn√©es brutes

La premi√®re √©tape consiste √† explorer le dataset initial afin de comprendre :

la structure des donn√©es,

le type des variables (num√©riques / cat√©gorielles),

la pr√©sence de valeurs manquantes,

la distribution des esp√®ces.

Le jeu de donn√©es contient initialement 344 observations et 17 variables, avec plusieurs valeurs manquantes, notamment dans :

la variable Sex,

les mesures morphologiques,

les mesures isotopiques,

et une colonne Comments tr√®s peu renseign√©e.

Cette √©tape permet d‚Äôidentifier les contraintes du dataset avant toute mod√©lisation.

2. Nettoyage des donn√©es

Une strat√©gie de nettoyage claire a √©t√© mise en place :

conservation uniquement des individus dont le sexe est identifi√© comme MALE ou FEMALE ;

suppression implicite des observations ambigu√´s ;

imputation des valeurs manquantes num√©riques par la m√©diane, m√©thode robuste face aux valeurs extr√™mes.

Apr√®s nettoyage, le dataset contient 333 individus propres, sans valeurs manquantes sur les variables utilis√©es pour la classification.

3. Analyse descriptive

Une analyse statistique descriptive a ensuite √©t√© r√©alis√©e afin de v√©rifier la pertinence du probl√®me de classification.

Cette analyse comprend :

statistiques globales (moyenne, √©cart-type, quartiles),

moyennes morphologiques par esp√®ce,

effectifs par esp√®ce,

tableaux crois√©s entre l‚Äô√Æle et l‚Äôesp√®ce,

histogrammes par esp√®ce.

Les r√©sultats montrent des diff√©rences morphologiques nettes, notamment :

les manchots Gentoo ont des nageoires et une masse corporelle significativement plus √©lev√©es ;

les esp√®ces Adelie et Chinstrap pr√©sentent des profils plus proches ;

certaines esp√®ces sont fortement li√©es √† certaines √Æles, ce qui apporte un fort pouvoir discriminant.

Ces observations expliquent en grande partie les performances √©lev√©es des mod√®les par la suite.

4. Pr√©traitement et pipeline

Les donn√©es contiennent des variables mixtes.

Le pr√©traitement est donc r√©alis√© √† l‚Äôaide d‚Äôune pipeline scikit-learn :

variables num√©riques :

imputation par la m√©diane,

standardisation (StandardScaler) ;

variables cat√©gorielles :

imputation par la modalit√© la plus fr√©quente,

encodage One-Hot.

L‚Äôensemble est int√©gr√© dans un ColumnTransformer, puis dans une Pipeline, ce qui garantit :

reproductibilit√©,

absence de fuite de donn√©es,

coh√©rence entre train et test.

Les donn√©es sont ensuite s√©par√©es via un train/test split 80/20 stratifi√©, afin de conserver les proportions de classes.

5. Mod√®les de classification classiques

Trois classifieurs supervis√©s ont √©t√© √©valu√©s :

Logistic Regression

Random Forest

Naive Bayes

Les performances sont √©valu√©es √† l‚Äôaide de :

balanced accuracy,

F1-score macro,

matrices de confusion.

R√©sultats principaux :

la r√©gression logistique obtient des performances parfaites ;

la for√™t al√©atoire commet une seule erreur ;

Naive Bayes est plus faible en raison de son hypoth√®se d‚Äôind√©pendance conditionnelle, non respect√©e ici.

Ces r√©sultats montrent l‚Äôimpact direct des hypoth√®ses statistiques des mod√®les sur leurs performances.

6. Classification non param√©trique par KDE

Une approche avanc√©e bas√©e sur les Kernel Density Estimators (KDE) a ensuite √©t√© impl√©ment√©e.

Le principe est le suivant :

estimation de la densit√© conditionnelle 
ùëù
(
ùë•
‚à£
ùë¶
)
p(x‚à£y) pour chaque esp√®ce ;

application de la r√®gle de Bayes pour la classification ;

utilisation d‚Äôun KDE multivari√© afin de ne pas supposer l‚Äôind√©pendance des variables.

Plusieurs noyaux ont √©t√© compar√©s :

gaussian,

epanechnikov,

exponential,

tophat.

Le param√®tre de lissage (bandwidth) est s√©lectionn√© via validation crois√©e stratifi√©e (GridSearchCV).

Les r√©sultats montrent :

des performances quasi parfaites pour la majorit√© des noyaux ;

une l√©g√®re sensibilit√© au choix du noyau (tophat l√©g√®rement moins performant).

Cette partie valide th√©oriquement et empiriquement l‚Äôapproche non param√©trique.

7. Comparaison avec LDA

Enfin, les r√©sultats sont compar√©s √† une Analyse Discriminante Lin√©aire (LDA).

LDA est un mod√®le g√©n√©ratif param√©trique supposant :

des distributions gaussiennes par classe,

une covariance commune.

Sur ce dataset, LDA atteint √©galement des performances parfaites, ce qui confirme que :

les distributions sont proches de gaussiennes,

les fronti√®res de d√©cision sont majoritairement lin√©aires.

Conclusion

Ce projet met en ≈ìuvre une d√©marche compl√®te de data science et de machine learning, allant :

de l‚Äôanalyse des donn√©es brutes,

au nettoyage raisonn√©,

√† l‚Äôexploration statistique,

√† la construction de pipelines robustes,

√† la comparaison de mod√®les param√©triques et non param√©triques.

Les principaux enseignements sont :

les diff√©rences morphologiques entre esp√®ces sont tr√®s marqu√©es ;

des mod√®les simples et interpr√©tables (Logistic Regression, LDA) suffisent √† atteindre des performances maximales ;

les m√©thodes plus flexibles (Random Forest, KDE) confirment ces r√©sultats ;

les hypoth√®ses statistiques jouent un r√¥le central dans la qualit√© des pr√©dictions.

Ce projet illustre ainsi l‚Äôimportance de comprendre les donn√©es avant de complexifier les mod√®les, et montre qu‚Äôun mod√®le bien choisi, m√™me simple, peut √™tre aussi performant qu‚Äôune m√©thode plus sophistiqu√©e.
